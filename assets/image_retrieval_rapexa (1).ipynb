{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "UZPkujelf0Xx",
        "outputId": "1dc9f593-5f7b-48bd-ddac-82b85471a3b5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dfc02724-b7b0-4eba-baf6-4f324b4fa2a6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dfc02724-b7b0-4eba-baf6-4f324b4fa2a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "! chmod 600 kaggle.json\n",
        "\n",
        "! kaggle datasets download -d theaayushbajaj/cbir-dataset --unzip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW44kNjMO-VO"
      },
      "outputs": [],
      "source": [
        "# کتابخانه اصلی مورد نیاز برای اجرای پروژه\n",
        "!pip install barbar torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsA887pYiuOF"
      },
      "outputs": [],
      "source": [
        "!pip install barbar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvOQJ1yWRsyB"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## بازیابی تصویر بر اساس محتوا (CBIR)\n",
        "## رویکرد:\n",
        "\n",
        "- با مشاهده داده ها کاملاً واضح است که استفاده از یک روش بدون نظارت همراه با چند رویکرد هشینگ مختلف می تواند نتایج خوبی را برای بازیابی تصویر بر اساس محتوا رقم بزند. اگرچه تعدادی تکنیک  دیگر در این زمینه نیز وجود دارند ولی ما بر روی تکنیک‌های هش و رمزگذاری خودکار تمرکز خواهیم کرد :\n",
        "    <br>\n",
        "\n",
        "    - ***استخراج ویژگی های پنهان تصاویر*** :\n",
        "    <br>\n",
        "    در این تکنیک می‌توانیم با ایجاد قلاب‌ها (hooks) در یک شبکه از پیش آموزش‌دیده و استخراج بردار از لایه‌های قبلی، بردارهای ویژگی را برای هر تصویر پیدا کنیم. تکنیک دیگر استفاده از AutoEncoders را ابداع می کند که در آن ویژگی های پنهان را می توان از خود رمزگذار استخراج کرد. و ما به خاطر این داده های موجود با AutoEncoders ادامه خواهیم داد. و برای بخش بازیابی تصاویر، به جستجوی مبتنی بر اقلیدسی (O(NlogN)) و رویکردهای مبتنی بر درهم سازی (O(logN)) خواهیم پرداخت.\n",
        "    \n",
        "    <br>\n",
        "    <br>\n",
        "\n",
        "    - ***جستجوی درهم سازی تصویر*** :\n",
        "    <br>\n",
        "\n",
        "    این می تواند توسط :\n",
        "        -  اینکه محتویات یک تصویر را تنها با استفاده از یک عدد صحیح کمی کنیم.\n",
        "        - یا اینکه تصاویر تکراری یا تقریباً تکراری را در مجموعه داده ای از تصاویر بر اساس هش های محاسبه شده آنها پیدا کنیم.<br>\n",
        "        <br>\n",
        "      این را می توانیم با یک ساختار داده تخصصی به نام **VP-Tree** انجام بدهیم. با استفاده از VP-Tree می‌توانیم پیچیدگی جستجوی خود را از O(nlogn) به O(log n) کاهش دهیم و به ما امکان می‌دهد به هدف زیر خطی خود دست پیدا کنیم!\n",
        "\n",
        "</div>\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GMfpSMZRB68"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchsummary import summary\n",
        "from pathlib import Path\n",
        "from barbar import Bar\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd # پردازش دادهه های درون فایل CSV I/O (e.g. pd.read_csv)\n",
        "import numpy as np # برای استفاده از جبر خطی\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import pickle\n",
        "import time\n",
        "import copy\n",
        "import scipy\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import gc\n",
        "\n",
        "RANDOMSTATE = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XwvUJ4ggaA9",
        "outputId": "89a04484-e6cf-4cdf-fb41-248162700285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# برای بهینه سازی سرعت اموزش مدل از cuda استفاده میکنیم اگر نبود از cpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uosZhth_ggal"
      },
      "outputs": [],
      "source": [
        "# آماده سازی DataFrame\n",
        "datasetPath = Path('/content/dataset/')\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n",
        "df['image'] = '/content/dataset/' + df['image'].astype(str)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOTRXGZ6g1t5"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "# آماده سازی داده ها\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7lT6gNUguem"
      },
      "outputs": [],
      "source": [
        "class CBIRDataset(Dataset):\n",
        "    def __init__(self, dataFrame):\n",
        "        self.dataFrame = dataFrame\n",
        "\n",
        "        self.transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, slice):\n",
        "            raise NotImplementedError('slicing is not supported')\n",
        "\n",
        "        row = self.dataFrame.iloc[key]\n",
        "        image = self.transformations(Image.open(row['image']))\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataFrame.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbzdXaXHgznQ"
      },
      "outputs": [],
      "source": [
        "# تابع میانی برای پردازش داده ها از کلاس بازیابی داده ها\n",
        "def prepare_data(DF):\n",
        "    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n",
        "    train_set = CBIRDataset(trainDF)\n",
        "    validate_set = CBIRDataset(validateDF)\n",
        "\n",
        "    return train_set, validate_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZTVYitqg99x"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "## مدل AutoEncoder\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y55WLirdhCIc"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "\n",
        "### ساختار سطح بالای یک AutoEncoder\n",
        "\n",
        "</dev>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js_tX5PrhIr_"
      },
      "source": [
        "---------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyUmOd0yg-k_"
      },
      "outputs": [],
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "\n",
        "            nn.Conv2d(in_channels=3,\n",
        "                      out_channels=16,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=3,\n",
        "                      padding=1),  # (32,16,171,171)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
        "\n",
        "            nn.Conv2d(in_channels=16,\n",
        "                      out_channels=8,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=2,\n",
        "                      padding=1),  # (N,8,43,43)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels = 8,\n",
        "                               out_channels=16,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=2),  # (N,16,85,85)\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=16,\n",
        "                               out_channels=8,\n",
        "                               kernel_size=(5,5),\n",
        "                               stride=3,\n",
        "                               padding=1),  # (N,8,255,255)\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=8,\n",
        "                               out_channels=3,\n",
        "                               kernel_size=(6,6),\n",
        "                               stride=2,\n",
        "                               padding=1),  # (N,3,512,512)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgv45ZJ1hQcm"
      },
      "outputs": [],
      "source": [
        "class ConvAutoencoder_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder_v2, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "\n",
        "            nn.Conv2d(in_channels=3,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                      out_channels=64,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=64,\n",
        "                      out_channels=128,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=128,\n",
        "                      out_channels=128,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=1,\n",
        "                      padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "            nn.Conv2d(in_channels=128,\n",
        "                      out_channels=256,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=2,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=256,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256,\n",
        "                      out_channels=256,\n",
        "                      kernel_size=(3,3),\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels = 256,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=1,\n",
        "                              padding=1),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=256,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=1,\n",
        "                               padding=1),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=256,\n",
        "                               out_channels=128,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=2,\n",
        "                               padding=0),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=128,\n",
        "                               out_channels=64,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=2,\n",
        "                               padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=64,\n",
        "                               out_channels=32,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=2,\n",
        "                               padding=1),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=32,\n",
        "                               out_channels=32,\n",
        "                               kernel_size=(3,3),\n",
        "                               stride=2,\n",
        "                               padding=1),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=32,\n",
        "                               out_channels=3,\n",
        "                               kernel_size=(4,4),\n",
        "                               stride=2,\n",
        "                               padding=2),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dODouZfGhVn7"
      },
      "outputs": [],
      "source": [
        "summary(ConvAutoencoder_v2().to(device),(3,512,512))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is1eAZBWhySR"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xreT4PWnhpzI"
      },
      "outputs": [],
      "source": [
        "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
        "\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    #valid_loss_min = checkpoint['valid_loss_min']\n",
        "\n",
        "    # return model, optimizer, epoch value, min validation loss\n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    print (\"=> Saving a new best\")\n",
        "    torch.save(state, filename)  # save checkpoint\n",
        "\n",
        "def train_model(model,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                #scheduler,\n",
        "                num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, inputs)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            #if phase == 'train':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, epoch_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                save_checkpoint(state={\n",
        "                                    'epoch': epoch,\n",
        "                                    'state_dict': model.state_dict(),\n",
        "                                    'best_loss': best_loss,\n",
        "                                    'optimizer_state_dict':optimizer.state_dict()\n",
        "                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, optimizer, epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-Ctoeo5h7Pi"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 150\n",
        "NUM_BATCHES = 32\n",
        "RETRAIN = False\n",
        "\n",
        "train_set, validate_set = prepare_data(DF=df)\n",
        "\n",
        "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n",
        "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n",
        "                }\n",
        "\n",
        "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
        "\n",
        "model = ConvAutoencoder_v2().to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "#exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FBDMWhRh703"
      },
      "outputs": [],
      "source": [
        "# If re-training is required:\n",
        "# Load the old model\n",
        "if RETRAIN == True:\n",
        "    # load the saved checkpoint\n",
        "    model, optimizer, start_epoch = load_ckpt('/content/conv_autoencoder.pt', model, optimizer)\n",
        "    print('Checkpoint Loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhnWuIGxieU6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "\n",
        "model, optimizer, loss = train_model(model=model,\n",
        "                    criterion=criterion,\n",
        "                    optimizer=optimizer,\n",
        "                    #scheduler=exp_lr_scheduler,\n",
        "                    num_epochs=EPOCHS)\n",
        "\n",
        "model.save('modelh5', save_format=\"h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOYPowLIoZ4R"
      },
      "outputs": [],
      "source": [
        "# Save the Trained Model\n",
        "torch.save({\n",
        "            'epoch': EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'conv_autoencoderv2_200ep.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGTEe12uWWx9"
      },
      "source": [
        "## Inference\n",
        "\n",
        "1. Indexing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poUkiE2zWV2I"
      },
      "outputs": [],
      "source": [
        "transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI8EJ_VLW1SJ"
      },
      "outputs": [],
      "source": [
        "# Load Model in Evaluation phase\n",
        "model = ConvAutoencoder_v2().to(device)\n",
        "model.load_state_dict(torch.load('/content/conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n",
        "\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ngnOmU_XFyR"
      },
      "outputs": [],
      "source": [
        "def get_latent_features(images, transformations):\n",
        "\n",
        "    latent_features = np.zeros((4738,256,16,16))\n",
        "    #latent_features = np.zeros((4738,8,42,42))\n",
        "\n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image)).to(device)\n",
        "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
        "\n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRt49CUrXPcm"
      },
      "outputs": [],
      "source": [
        "images = df.image.values\n",
        "latent_features = get_latent_features(images, transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiyHhQWZXQfY"
      },
      "outputs": [],
      "source": [
        "indexes = list(range(0, 4738))\n",
        "feature_dict = dict(zip(indexes,latent_features))\n",
        "index_dict = {'indexes':indexes,'features':latent_features}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIfXrgjzXTQl"
      },
      "outputs": [],
      "source": [
        "# write the data dictionary to disk\n",
        "with open('features.pkl', \"wb\") as f:\n",
        "    f.write(pickle.dumps(index_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Rs-IFgXaJL"
      },
      "source": [
        "2. Image Retrieval¶\n",
        "This will be approached with two ways as discussed in the start:\n",
        "\n",
        "- Euclidean Search:\n",
        "    - Identifying the Latent Features\n",
        "    - Calculating the Euclidean Distance between them\n",
        "    - Returning the closest N indexes (of images)\n",
        "\n",
        "- Locality Sensitive Hashing\n",
        "    - Create hashes of the feature vector from Encoder\n",
        "    - Store it in a Hashing Table\n",
        "    - Identify closest images based on hamming distance\n",
        "<br/>\n",
        "- 2.1 Euclidean Search Method¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J368TMSJXoZ-"
      },
      "outputs": [],
      "source": [
        "def euclidean(a, b):\n",
        "    # compute and return the euclidean distance between two vectors\n",
        "    return np.linalg.norm(a - b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2-k1x26XpTo"
      },
      "outputs": [],
      "source": [
        "def cosine_distance(a,b):\n",
        "    return scipy.spatial.distance.cosine(a, b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNQzRYoXXq5S"
      },
      "outputs": [],
      "source": [
        "def perform_search(queryFeatures, index, maxResults=64):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(index[\"features\"])):\n",
        "        # compute the euclidean distance between our query features\n",
        "        # and the features for the current image in our index, then\n",
        "        # update our results list with a 2-tuple consisting of the\n",
        "        # computed distance and the index of the image\n",
        "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
        "        results.append((d, i))\n",
        "\n",
        "    # sort the results and grab the top ones\n",
        "    results = sorted(results)[:maxResults]\n",
        "    # return the list of results\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEUZkc0sXtCg"
      },
      "outputs": [],
      "source": [
        "def build_montages(image_list, image_shape, montage_shape):\n",
        "\n",
        "    if len(image_shape) != 2:\n",
        "        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n",
        "    if len(montage_shape) != 2:\n",
        "        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n",
        "    image_montages = []\n",
        "    # start with black canvas to draw images onto\n",
        "    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                          dtype=np.uint8)\n",
        "    cursor_pos = [0, 0]\n",
        "    start_new_img = False\n",
        "    for img in image_list:\n",
        "        if type(img).__module__ != np.__name__:\n",
        "            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n",
        "        start_new_img = False\n",
        "        img = cv2.resize(img, image_shape)\n",
        "        # draw image to black canvas\n",
        "        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n",
        "        cursor_pos[0] += image_shape[0]  # increment cursor x position\n",
        "        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n",
        "            cursor_pos[1] += image_shape[1]  # increment cursor y position\n",
        "            cursor_pos[0] = 0\n",
        "            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n",
        "                cursor_pos = [0, 0]\n",
        "                image_montages.append(montage_image)\n",
        "                # reset black canvas\n",
        "                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                                      dtype=np.uint8)\n",
        "                start_new_img = True\n",
        "    if start_new_img is False:\n",
        "        image_montages.append(montage_image)  # add unfinished montage\n",
        "    return image_montages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBJnOMcNXwBO"
      },
      "outputs": [],
      "source": [
        "# take the features for the current image, find all similar\n",
        "# images in our dataset, and then initialize our list of result\n",
        "# images\n",
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "queryIdx = 3166# Input Index for which images\n",
        "MAX_RESULTS = 10\n",
        "\n",
        "\n",
        "queryFeatures = latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hymVyEZGXyTT"
      },
      "outputs": [],
      "source": [
        "testpath = Path('../input/testcbir/Test_Images')\n",
        "testdf = pd.DataFrame()\n",
        "\n",
        "testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n",
        "testdf['image'] = '../input/testcbir/Test_Images/' + testdf['image'].astype(str)\n",
        "\n",
        "testdf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dazySFKX00Z"
      },
      "outputs": [],
      "source": [
        "testimages = testdf.image.values\n",
        "test_latent_features = get_latent_features(testimages, transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPLgRkyVX36Z"
      },
      "outputs": [],
      "source": [
        "test_latent_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEo0tuGfX7S3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "MAX_RESULTS = 10\n",
        "queryIdx = 12\n",
        "\n",
        "queryFeatures = test_latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7OAIucmYDgA"
      },
      "source": [
        "## 2.2 LSHashing Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khxzB8zhX-b_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1eab6c-1161-4a47-b6c8-d8503c90d104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lshashpy3\n",
            "  Downloading lshashpy3-0.0.9.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from lshashpy3) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from lshashpy3) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lshashpy3) (1.26.4)\n",
            "Collecting bitarray (from lshashpy3)\n",
            "  Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Downloading bitarray-3.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: lshashpy3\n",
            "  Building wheel for lshashpy3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lshashpy3: filename=lshashpy3-0.0.9-py3-none-any.whl size=8880 sha256=71424d6c0323fdaa2d384e78312bffba04bb7f871eb037f9450f9b11f35ad3ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/be/c3/df8585bbabf5752de55341b1728dfed22c96349c300bac76f3\n",
            "Successfully built lshashpy3\n",
            "Installing collected packages: bitarray, lshashpy3\n",
            "Successfully installed bitarray-3.0.0 lshashpy3-0.0.9\n"
          ]
        }
      ],
      "source": [
        "## ---------------------------------------------------\n",
        "!pip install lshashpy3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMB5jZC_YJBu"
      },
      "outputs": [],
      "source": [
        "from lshashpy3 import LSHash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaJ3Ks5HYKxM"
      },
      "outputs": [],
      "source": [
        "# Locality Sensitive Hashing\n",
        "params\n",
        "k = 12 # hash size\n",
        "L = 5  # number of tables\n",
        "d = 14112 # Dimension of Feature vector\n",
        "lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n",
        "\n",
        "# LSH on all the images\n",
        "for idx,vec in tqdm(feature_dict.items()):\n",
        "    lsh.index(vec.flatten(), extra_data=idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF8BAazIYNnv"
      },
      "outputs": [],
      "source": [
        "# Exporting as pickle\n",
        "pickle.dump(lsh, open('lsh.p', \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1hUDsXPYQid"
      },
      "outputs": [],
      "source": [
        "def get_similar_item(idx, feature_dict, lsh_variable, n_items=10):\n",
        "    response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(),\n",
        "                     num_results=n_items+1, distance_func='hamming')\n",
        "\n",
        "    imgs = []\n",
        "    for i in range(1, n_items+1):\n",
        "        imgs.append(np.array(Image.open(images[response[i][0][1]])))\n",
        "    return imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xevVsxSKYSzw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "queryIdx = 5\n",
        "\n",
        "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "montage = build_montages(get_similar_item(queryIdx, feature_dict, lsh,10),(512, 512), (5, 2))[0]\n",
        "ax[1].imshow(montage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGGw_i5oYWW_"
      },
      "source": [
        "# End Notes\n",
        "\n",
        "We started with the approach of AutoEncoders for Image Latent Features extraction followed by Image retrieval using Euclidean Distance which was an O(NlogN) approach (Time-Complexity) to Hashing which gave us an ~O(logN) approach\n",
        "\n",
        "Another approach was to use Hashing on features obtained from SIFT, SURF, OBS and building the VP Trees ans search the images in it.\n",
        "\n",
        "</br>\n",
        "--------------------------------------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMbI4NA1Y2iE"
      },
      "source": [
        "# Clustering of Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "506eGbdNY5YA"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2-9k4SPY7QE"
      },
      "outputs": [],
      "source": [
        "def get_latent_features1D(images, transformations):\n",
        "\n",
        "    latent_features1d = []\n",
        "\n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image)).to(device)\n",
        "        latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n",
        "\n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5DVgOdGY-nc"
      },
      "outputs": [],
      "source": [
        "images = df.image.values\n",
        "latent_features1d = get_latent_features1D(images, transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5V1ZfKHZAiH"
      },
      "outputs": [],
      "source": [
        "latent_features1d = np.array(latent_features1d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MkdaimGZCnL"
      },
      "outputs": [],
      "source": [
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(4,10)\n",
        "\n",
        "for k in tqdm(K):\n",
        "    #Building and fitting the model\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_,\n",
        "                      'euclidean'),axis=1)) / latent_features1d.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "\n",
        "    mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_,\n",
        "                 'euclidean'),axis=1)) / latent_features1d.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjzJeK-hZGVE"
      },
      "outputs": [],
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S0pYfxTZImP"
      },
      "outputs": [],
      "source": [
        "X = np.array(latent_features1d)\n",
        "K = range(3,10)\n",
        "\n",
        "for n_clusters in tqdm(K):\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGUak6i5ZMg4"
      },
      "source": [
        "</br>\n",
        "\n",
        "*   The Silhouette score isn't significant for any cluster since its close to 0 for every k, that translates to less differentiability for a point to belong to a particular cluster.\n",
        "\n",
        "</br>\n",
        "\n",
        "*   GMM can help in this case because animals share a lot of similar traits with each other in terms of appearance but we have to get the bottleneck case since an animal can only belong to one cluster, so kmeans will be the way to go but a different feature/keypoint detection might help identify right number of clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i902R33XZgI1"
      },
      "source": [
        "# Using SIFT/SURF/ORB technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5sOxp1qZjkW"
      },
      "outputs": [],
      "source": [
        "def build_dictionary(xfeatures2d, images, n_clusters):\n",
        "    #print('Computing descriptors..')\n",
        "    desc_list = []\n",
        "\n",
        "    for image_path in images:\n",
        "        image = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n",
        "        desc_list.extend(dsc)\n",
        "\n",
        "    desc = np.array(desc_list)\n",
        "    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n",
        "    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n",
        "    dictionary.fit(desc)\n",
        "\n",
        "    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_,\n",
        "                      'euclidean'),axis=1)) / desc.shape[0]\n",
        "\n",
        "    return distortion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egLDMq2HZmHC"
      },
      "outputs": [],
      "source": [
        "orb = cv2.ORB_create()\n",
        "images = df.image.values\n",
        "K = range(4,10)\n",
        "distortions = []\n",
        "\n",
        "for k in tqdm(K):\n",
        "    distortions.append(build_dictionary(orb, images, n_clusters=k))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX3KxNECZo2Z"
      },
      "outputs": [],
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method using Distortion')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaO57bydZtBQ"
      },
      "source": [
        "*   The ORB technique tells us there are 6/7 major clusters that are persistent in the data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}